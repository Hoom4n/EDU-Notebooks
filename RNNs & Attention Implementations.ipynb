{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b116852d-e1d2-4b12-9ba8-507aaf49f80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1658cf8-3c68-403f-b620-c2dae0d3e7d7",
   "metadata": {},
   "source": [
    "## Simple RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6aae5e6-f661-46cb-84d8-19787389b110",
   "metadata": {},
   "source": [
    "**Simple RNN Memory Cell**\n",
    "\n",
    "We can do computations in one shot for the whole layer:\n",
    "$$\n",
    "\\hat{Y}_t = \\phi(X_t W_x + \\hat{Y}_{t-1} W_{\\hat{y}} + b)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\hat{Y}_t$ is an $m \\times n_{\\text{neurons}}$ matrix containing the layer’s outputs at time step $t$ for each instance in the mini-batch ($m$ is the number of instances in the mini-batch and $n_{\\text{neurons}}$ is the number of neurons).  \n",
    "- $X_t$ is an $m \\times n_{\\text{inputs}}$ matrix containing the inputs for all instances ($n_{\\text{inputs}}$ is the number of input features).  \n",
    "- $W_x$ is an $n_{\\text{inputs}} \\times n_{\\text{neurons}}$ matrix containing the connection weights for the inputs of the current time step.  \n",
    "- $W_{\\hat{y}}$ is an $n_{\\text{neurons}} \\times n_{\\text{neurons}}$ matrix containing the connection weights for the outputs of the previous time step.  \n",
    "- $b$ is a vector of size $n_{\\text{neurons}}$ containing each neuron’s bias term.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9dab333-3446-4d60-a706-0a8a56f5471c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def simple_rnn_cell(x_t, h_t_1, w_x, w_h, b):\n",
    "    return tf.tanh(x_t @ w_x + h_t_1 @ w_h + b) # tensor in, tensor out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9abe2ab6-7fde-4be2-abd3-573c69bbbeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "units = 8\n",
    "n_features = 12\n",
    "timesteps = 16\n",
    "\n",
    "kernel_init = keras.initializers.glorot_uniform(seed=42) # best for tanh activation\n",
    "recurrent_init = keras.initializers.orthogonal(seed=42) # best for recurrent weights\n",
    "\n",
    "w_x = kernel_init(shape=(n_features, units))\n",
    "w_h = recurrent_init(shape=(units, units))\n",
    "b = tf.zeros(shape=(units)) # 1 bias per neuron\n",
    "h_init = tf.zeros(shape=(batch_size,units)) # if this was stateful rnn, we fed last hidden state of sequence before instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a1947ba-f472-46da-b5ba-5df2cb25dacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "input_sequence = tf.random.normal(shape=(batch_size,timesteps,n_features)) # typical 3D rnn input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2db9a718-a87d-48fc-b860-602f0eb9ca9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 8])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_t = input_sequence[:,0,:] # each timestep x(t) shape: (batch_size, n_features)\n",
    "h_t = simple_rnn_cell(x_t, h_init, w_x, w_h, b) # returns one hidden state per sample-neuron: (batch_size, units)\n",
    "y_t = h_t # in simple rnn, timestep t's output is equal to its hidden state\n",
    "y_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ceb68a2-7de3-4c23-a97c-e93838857212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 16, 8])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_t_1 = h_init\n",
    "hiddens_states = []\n",
    "\n",
    "for t in range(timesteps):\n",
    "    x_t = input_sequence[:,t,:] \n",
    "    h_t = simple_rnn_cell(x_t, h_t_1, w_x, w_h, b)\n",
    "    hiddens_states.append(h_t)\n",
    "    h_t_1 = h_t\n",
    "\n",
    "outputs = tf.stack(hiddens_states, axis=1) # stacked on axis=1, timesteps\n",
    "outputs.shape # returns (batch_size, timesteps, units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a380bc54-eccd-4f14-830b-2589750ea20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets wrap eveything in a function\n",
    "\n",
    "def simple_rnn(input_sequence, units, return_sequences=True, seed=42, h_init=None):\n",
    "    assert input_sequence.ndim == 3, \"RNN expects 3D input\"\n",
    "    batch_size, timesteps, n_features = input_sequence.shape\n",
    "\n",
    "    kernel_init = keras.initializers.glorot_uniform(seed=seed)\n",
    "    recurrent_init = keras.initializers.orthogonal(seed=seed)\n",
    "    w_x = kernel_init(shape=(n_features, units))\n",
    "    w_h = recurrent_init(shape=(units, units))\n",
    "    b = tf.zeros(shape=(units)) \n",
    "    h_init = tf.zeros(shape=(batch_size,units)) if h_init is None else h_init\n",
    "\n",
    "    h_t_1 = h_init\n",
    "    hiddens_states = []\n",
    "\n",
    "    for t in range(timesteps):\n",
    "        x_t = input_sequence[:,t,:] \n",
    "        h_t = simple_rnn_cell(x_t, h_t_1, w_x, w_h, b)\n",
    "        hiddens_states.append(h_t)\n",
    "        h_t_1 = h_t\n",
    "    \n",
    "    outputs = tf.stack(hiddens_states, axis=1)\n",
    "    \n",
    "    return outputs if return_sequences else outputs[:,-1,:] # or else return last timestep output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "735f752a-dde9-4160-9941-8d1a082ce424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are my results same as Keras layer? True\n"
     ]
    }
   ],
   "source": [
    "my_simple_rnn_outputs = simple_rnn(input_sequence, units, return_sequences=True, seed=42)\n",
    "\n",
    "keras_simple_rnn = keras.layers.SimpleRNN(units,\n",
    "                                          return_sequences=True, \n",
    "                                          kernel_initializer=kernel_init, \n",
    "                                          recurrent_initializer=recurrent_init)\n",
    "\n",
    "keras_simple_rnn_outputs = keras_simple_rnn(input_sequence)\n",
    "\n",
    "print(f\"Are my results same as Keras layer? {(my_simple_rnn_outputs == keras_simple_rnn_outputs).numpy().all()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304c4fff-5c54-4529-a7b7-ab7b25e0cc65",
   "metadata": {},
   "source": [
    "Yay!\n",
    "\n",
    "Some Notes:\n",
    "- Although those parameters need to be set up somehow so the optimizer can tweak them, I just implemented the forward pass to get the intuition.\n",
    "- A single Simple RNN cell outputs a tensor of shape (batch_size, features) at each time step, where the dimensionality of features equals the number of units. So essentially, at each time step, the hidden state of a single unit is a scalar—meaning all the encoded information up to that point is compressed into a single number! Given the kinds of tasks RNNs can handle (speech recognition, image captioning, language modeling), this is quite fascinating!\n",
    "- If the gradients adjust the weights in a way that slightly increases them, it can lead to trouble—because all time steps share the same weight matrix, those small changes can accumulate and eventually cause the hidden states to explode. That’s one reason we use tanh, a saturated (bounded) activation function, instead of ReLU, which is unbounded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7455323d-79fb-4e04-adbf-4eedbb72f9a6",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02d2971-a7e9-4ef5-b69b-949354a2f93e",
   "metadata": {},
   "source": [
    "### Implementing LSTM Memory Cell: Step by Step in TensorFlow Eager Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d291ff-b64f-443c-9c02-f302b905b27d",
   "metadata": {},
   "source": [
    "According to Wikipedia:\n",
    "\n",
    "- $f_t = \\sigma(W_f x_t + U_f h_{t-1} + b_f)$  \n",
    "- $i_t = \\sigma(W_i x_t + U_i h_{t-1} + b_i)$  \n",
    "- $\\tilde{c}_t = \\tanh(W_c x_t + U_c h_{t-1} + b_c)$  \n",
    "- $o_t = \\sigma(W_o x_t + U_o h_{t-1} + b_o)$ \n",
    "- $c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t$\n",
    "- $h_t = o_t \\odot \\tanh(c_t)$ \n",
    "\n",
    "\n",
    "Initial values are $c_0 = 0$ \n",
    "and $h_0 = 0$, and the operator $\\odot$ denotes the Hadamard product (element-wise multiplication); the subscript $t$ indexes the time step.\n",
    "\n",
    "Letting the superscripts $d$ and $h$ refer to the number of input features and number of hidden units, respectively:\n",
    "\n",
    "- $x_t \\in \\mathbb{R}^d$: input vector to the LSTM unit  \n",
    "- $f_t \\in (0,1)^h$: forget gate's activation vector  \n",
    "- $i_t \\in (0,1)^h$: input/update gate's activation vector  \n",
    "- $o_t \\in (0,1)^h$: output gate's activation vector  \n",
    "- $h_t \\in (-1,1)^h$: hidden state vector (also output of the LSTM unit)  \n",
    "- $\\tilde{c}_t \\in (-1,1)^h$: cell input activation vector  \n",
    "- $c_t \\in \\mathbb{R}^h$: cell state vector  \n",
    "- $W \\in \\mathbb{R}^{h \\times d}$, $U \\in \\mathbb{R}^{h \\times h}$, $b \\in \\mathbb{R}^h$: weight matrices and bias vector  \n",
    "- $\\sigma_g$: sigmoid activation function  \n",
    "- $\\sigma_c$: hyperbolic tangent activation function\n",
    "\n",
    "these equations are for single sample, i will implement batched operations, so X_t is shaped: (batch_size, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0c741f3-750b-43fb-a9cd-a2a20f21b3aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 12])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "timesteps = 16\n",
    "n_features = 12\n",
    "units = 8\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "X = tf.random.normal(shape=(batch_size, timesteps, n_features)) # X batch: (batch_size, timesteps, n_features)\n",
    "x_t = X[:,0,:]\n",
    "x_t.shape # x_t shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39eb2384-9778-403c-8c80-8ff9c53949f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for now i have to initialize them seprately, but later on i will concatanate them and initialzie each of W, U once\n",
    "w_f = keras.initializers.GlorotUniform(seed=43)(shape=(units, n_features))\n",
    "w_i = keras.initializers.GlorotUniform(seed=44)(shape=(units, n_features))\n",
    "w_o = keras.initializers.GlorotUniform(seed=45)(shape=(units, n_features))\n",
    "w_c = keras.initializers.GlorotUniform(seed=46)(shape=(units, n_features))\n",
    "\n",
    "u_f = keras.initializers.Orthogonal(seed=53)(shape=(units, units))\n",
    "u_i = keras.initializers.Orthogonal(seed=54)(shape=(units, units))\n",
    "u_o = keras.initializers.Orthogonal(seed=55)(shape=(units, units))\n",
    "u_c = keras.initializers.Orthogonal(seed=56)(shape=(units, units))\n",
    "\n",
    "h_init = tf.zeros(shape=(batch_size, units))\n",
    "c_init = tf.zeros(shape=(batch_size, units))\n",
    "\n",
    "b_f = tf.ones(shape=(units)) # tensorflow init forget bias with ones, this prevents forgetting everything at the beginning of training\n",
    "b_i = tf.zeros(shape=(units))\n",
    "b_o = tf.zeros(shape=(units))\n",
    "b_c = tf.zeros(shape=(units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98fb18b5-cb28-4a1c-ab0f-8cc45775729c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 8])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x_t is (batch_size, n_features) and w_f is (units, n_features) -> matmul -> (batch_size, units)\n",
    "# h_t_1 is (batch_size, units) and u_f is (units, units) -> matmul -> (batch_size, units)\n",
    "# b is (units) -> broadcast and add\n",
    "\n",
    "h_t_1 = h_init\n",
    "\n",
    "f_t = tf.sigmoid(x_t @ tf.transpose(w_f)  + h_t_1 @ u_f + b_f) \n",
    "f_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f1f8db-03c3-4726-be28-c5087a92184b",
   "metadata": {},
   "source": [
    "Alright, \n",
    "𝑓\n",
    "(\n",
    "𝑡\n",
    ")\n",
    " outputs \n",
    "(\n",
    "batch_size\n",
    ",\n",
    " units\n",
    ")\n",
    ". So for each sample and each unit, at each timestep, the forget activation is a number between 0 and 1 that determines how much of the memory cell state should be forgotten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "421da85e-8204-472b-b89c-fe2ac4e34cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of Input activation: (32, 8) and Output activation: (32, 8)\n"
     ]
    }
   ],
   "source": [
    "# same thing here\n",
    "\n",
    "i_t = tf.sigmoid(x_t @ tf.transpose(w_i)  + h_t_1 @ u_i + b_i) \n",
    "o_t = tf.sigmoid(x_t @ tf.transpose(w_o)  + h_t_1 @ u_o + b_o) \n",
    "\n",
    "print(f\"Shapes of Input activation: {i_t.shape} and Output activation: {o_t.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "293e7f26-c955-41e3-89d3-eb7ed54af9f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 8])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now c candiate. linear transformation + tanh, some of it will be added to long term memory\n",
    "c_candidate_t = tf.tanh(x_t @ tf.transpose(w_c)  + h_t_1 @ u_c + b_c) \n",
    "c_candidate_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20ca5138-6f38-4f55-a495-1fdbf4d0be18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 8])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now, forget and input gates: determine how much of c(t-1) should be forgotten and how much of the candidate cell state (c_candidate_t) should be added to it\n",
    "\n",
    "# forget gate: f_t (batch_size, units) * c_t_1 (batch_size, units)\n",
    "# input gate: i_t (batch_size, units) * c_candidate_t (batch_size, units)\n",
    "\n",
    "c_t_1 = c_init # initialize cell state with zeros\n",
    "\n",
    "c_t = f_t * c_t_1 + i_t * c_candidate_t\n",
    "\n",
    "c_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7b09105-8b5e-429f-afd6-ce3b684f4717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 8])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hiddens state h(t) (same as cell's output y(t)): for time step t is some of activated cell state\n",
    "\n",
    "h_t = y_t = o_t * tf.tanh( c_t )\n",
    "\n",
    "h_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d9c4b4-3ffa-4b7c-afe0-af11b89a4a9c",
   "metadata": {},
   "source": [
    "That's it!  \n",
    "This was for a single memory cell, so now let's write a function to handle all timesteps. We'll also concatenate the weight matrices and biases, then perform the matrix multiplications in one shot to take advantage of vectorization for a speed boost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce3b407-5f5c-4acb-945a-de49e74d4374",
   "metadata": {},
   "source": [
    "### Vectorizerd Implementation of LSTM Layer in TF Graph Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "832b4147-86e7-462d-ab15-db7cf5b8f0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacking all w matrices vertically, along first dimension -> W: (4 * units, n_features)\n",
    "# stacking all u matrices vertically, along first dimension -> U: (4 * units, units)\n",
    "# x_t: (batch_size, n_features)\n",
    "# h_t_1: (batch_size, units)\n",
    "# c_t_1: (batch_size, units)\n",
    "# b vector is repeated 4 times horizonaltly, -> B: (4 * units)\n",
    "\n",
    "@tf.function\n",
    "def lstm_cell(x_t, h_t_1, c_t_1, W, U, B):\n",
    "    z = x_t @ W + h_t_1 @ U + B # compute all 4 equations in one shot!\n",
    "    i_t, f_t, c_candidate_t, o_t = tf.split(z, 4, axis=1)\n",
    "    \n",
    "    f_t = tf.sigmoid(f_t)\n",
    "    i_t = tf.sigmoid(i_t)\n",
    "    o_t = tf.sigmoid(o_t)\n",
    "    c_candidate_t = tf.tanh(c_candidate_t)\n",
    "    \n",
    "    c_t = f_t * c_t_1 + i_t * c_candidate_t\n",
    "    h_t = o_t * tf.tanh(c_t)\n",
    "    return h_t, c_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53abc18c-616c-47c8-8eed-3450664ee097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm(input_sequence, units, kernel_initializer, recurrent_initializer, return_sequences=True, h_init=None, c_init=None, return_state=False):\n",
    "    assert input_sequence.ndim == 3, \"LSTM expects 3D input\"\n",
    "    batch_size, timesteps, n_features = input_sequence.shape\n",
    "\n",
    "    W_init = kernel_init(shape=(n_features, 4 * units)) \n",
    "    U_init = recurrent_init(shape=(units, 4 * units))   \n",
    "    \n",
    "    b_init = tf.concat([\n",
    "        tf.zeros(units),  # Input gate\n",
    "        tf.ones(units),   # Forget gate\n",
    "        tf.zeros(units),  # Cell candidate\n",
    "        tf.zeros(units)   # Output gate\n",
    "                        ], axis=0)\n",
    "    \n",
    "    h_init = tf.zeros(shape=(batch_size,units)) if h_init is None else h_init\n",
    "    c_init = tf.zeros(shape=(batch_size,units)) if c_init is None else c_init\n",
    "    \n",
    "    W = W_init\n",
    "    U = U_init\n",
    "    B = b_init\n",
    "    h_t_1 = h_init \n",
    "    c_t_1 = c_init\n",
    "\n",
    "    hiddens_states = []\n",
    "    cell_states = []\n",
    "    \n",
    "    for t in range(timesteps):\n",
    "        x_t = input_sequence[:,t,:] \n",
    "        h_t, c_t = lstm_cell(x_t, h_t_1, c_t_1, W, U, B)\n",
    "        \n",
    "        hiddens_states.append(h_t)\n",
    "        cell_states.append(c_t)\n",
    "        \n",
    "        h_t_1 = h_t\n",
    "        c_t_1 = c_t\n",
    "    \n",
    "    layer_hidden_states = tf.stack(hiddens_states, axis=1)\n",
    "\n",
    "    if return_state:\n",
    "        return layer_hidden_states, h_t, c_t\n",
    "    elif return_sequences:\n",
    "        return layer_hidden_states  \n",
    "    else:\n",
    "        return layer_hidden_states[:, -1, :]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d7bbab1-0436-4b4d-900f-23e49b9879aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "input_sequence = tf.random.normal(shape=(batch_size,timesteps,n_features)) \n",
    "\n",
    "kernel_init = keras.initializers.glorot_uniform(seed=42)\n",
    "recurrent_init = keras.initializers.orthogonal(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb105e64-065d-477f-9dda-a924b5fd79ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are they same?: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "my_lstm_hidden_states = lstm(input_sequence, units, kernel_init, recurrent_init, return_sequences=True, h_init=None, c_init=None, return_state=False)\n",
    "\n",
    "keras_lstm_layer = keras.layers.LSTM(units,\n",
    "                                     kernel_initializer=kernel_init, recurrent_initializer=recurrent_init,\n",
    "                                     return_sequences=True, return_state=False, use_cudnn=False)\n",
    "\n",
    "keras_lstm_hidden_states = keras_lstm_layer(input_sequence)\n",
    "\n",
    "print(f\"Are they same?: {(my_lstm_hidden_states == keras_lstm_hidden_states).numpy().all()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3528a3-6b37-43b4-8e70-da1c4923d421",
   "metadata": {},
   "source": [
    "Yayy!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6c46f7-3b02-4af2-8a3f-dbd0fe7bbb39",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af0e754-0cab-4dd9-9d6f-66fedaf5e229",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Attention (Concatanative)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812176a6-56fc-4d4e-bc71-604f16c69381",
   "metadata": {},
   "source": [
    "## Attention (Luong)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
