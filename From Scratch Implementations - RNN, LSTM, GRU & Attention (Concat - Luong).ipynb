{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b116852d-e1d2-4b12-9ba8-507aaf49f80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1658cf8-3c68-403f-b620-c2dae0d3e7d7",
   "metadata": {},
   "source": [
    "## Simple RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6aae5e6-f661-46cb-84d8-19787389b110",
   "metadata": {},
   "source": [
    "**1. Simple RNN Memory Cell**\n",
    "\n",
    "$$\n",
    "\\hat{Y}_t = \\phi(X_t W_x + \\hat{Y}_{t-1} W_{\\hat{y}} + b)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\hat{Y}_t$ is an $m \\times n_{\\text{neurons}}$ matrix containing the layer’s outputs at time step $t$ for each instance in the mini-batch ($m$ is the number of instances in the mini-batch and $n_{\\text{neurons}}$ is the number of neurons).  \n",
    "- $X_t$ is an $m \\times n_{\\text{inputs}}$ matrix containing the inputs for all instances ($n_{\\text{inputs}}$ is the number of input features).  \n",
    "- $W_x$ is an $n_{\\text{inputs}} \\times n_{\\text{neurons}}$ matrix containing the connection weights for the inputs of the current time step.  \n",
    "- $W_{\\hat{y}}$ is an $n_{\\text{neurons}} \\times n_{\\text{neurons}}$ matrix containing the connection weights for the outputs of the previous time step.  \n",
    "- $b$ is a vector of size $n_{\\text{neurons}}$ containing each neuron’s bias term.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9dab333-3446-4d60-a706-0a8a56f5471c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def rnn_cell(x_t, h_t_1, w_x, w_h, b):\n",
    "    return tf.tanh(x_t @ w_x + h_t_1 @ w_h + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9abe2ab6-7fde-4be2-abd3-573c69bbbeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "units = 8\n",
    "n_features = 12\n",
    "timesteps = 16\n",
    "\n",
    "kernel_init = keras.initializers.glorot_uniform(seed=42) # best for tanh activation\n",
    "recurrent_init = keras.initializers.orthogonal(seed=42) # best for recurrent weights\n",
    "\n",
    "w_x = kernel_init(shape=(n_features, units))\n",
    "w_h = recurrent_init(shape=(units, units))\n",
    "b = tf.zeros(shape=(units)) # 1 bias per neuron\n",
    "h_init = tf.zeros(shape=(batch_size,units)) # if this was stateful rnn, we fed last hidden state of sequence before instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a1947ba-f472-46da-b5ba-5df2cb25dacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "input_sequence = tf.random.normal(shape=(batch_size,timesteps,n_features)) # typical 3D rnn input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2db9a718-a87d-48fc-b860-602f0eb9ca9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 8])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_t = input_sequence[:,0,:] # each timestep x(t) shape: (batch_size, n_features)\n",
    "h_t = rnn_cell(x_t, h_init, w_x, w_h, b) # returns one hidden state per sample-neuron: (batch_size, units)\n",
    "y_t = h_t # in simple rnn, timestep t's output is equal to its hidden state\n",
    "y_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ceb68a2-7de3-4c23-a97c-e93838857212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 16, 8])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_t_1 = h_init\n",
    "hiddens_states = []\n",
    "\n",
    "for t in range(timesteps):\n",
    "    x_t = input_sequence[:,t,:] \n",
    "    h_t = rnn_cell(x_t, h_t_1, w_x, w_h, b)\n",
    "    hiddens_states.append(h_t)\n",
    "    h_t_1 = h_t\n",
    "\n",
    "outputs = tf.stack(hiddens_states, axis=1) # stacked on axis=1, timesteps\n",
    "outputs.shape # returns (batch_size, timesteps, units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a380bc54-eccd-4f14-830b-2589750ea20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets wrap eveything in a function\n",
    "\n",
    "def simple_rnn(input_sequence, units, return_sequences=True, seed=42):\n",
    "    assert input_sequence.ndim == 3, \"RNN expects 3D input\"\n",
    "    batch_size, timesteps, n_features = input_sequence.shape\n",
    "\n",
    "    kernel_init = keras.initializers.glorot_uniform(seed=seed)\n",
    "    recurrent_init = keras.initializers.orthogonal(seed=seed)\n",
    "    w_x = kernel_init(shape=(n_features, units))\n",
    "    w_h = recurrent_init(shape=(units, units))\n",
    "    b = tf.zeros(shape=(units)) \n",
    "    h_init = tf.zeros(shape=(batch_size,units))\n",
    "\n",
    "    h_t_1 = h_init\n",
    "    hiddens_states = []\n",
    "\n",
    "    for t in range(timesteps):\n",
    "        x_t = input_sequence[:,t,:] \n",
    "        h_t = rnn_cell(x_t, h_t_1, w_x, w_h, b)\n",
    "        hiddens_states.append(h_t)\n",
    "        h_t_1 = h_t\n",
    "    \n",
    "    outputs = tf.stack(hiddens_states, axis=1)\n",
    "    \n",
    "    return outputs if return_sequences else outputs[:,-1,:] # or else return last timestep output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "735f752a-dde9-4160-9941-8d1a082ce424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are my results same as keras layer? True\n"
     ]
    }
   ],
   "source": [
    "my_simple_rnn_outputs = simple_rnn(input_sequence, units, return_sequences=True, seed=42)\n",
    "\n",
    "keras_simple_rnn = keras.layers.SimpleRNN(units,\n",
    "                                          return_sequences=True, \n",
    "                                          kernel_initializer=kernel_init, \n",
    "                                          recurrent_initializer=recurrent_init)\n",
    "\n",
    "keras_simple_rnn_outputs = keras_simple_rnn(input_sequence)\n",
    "\n",
    "print(f\"Are my results same as keras layer? {(my_simple_rnn_outputs == keras_simple_rnn_outputs).numpy().all()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304c4fff-5c54-4529-a7b7-ab7b25e0cc65",
   "metadata": {},
   "source": [
    "Yay!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7455323d-79fb-4e04-adbf-4eedbb72f9a6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6c46f7-3b02-4af2-8a3f-dbd0fe7bbb39",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af0e754-0cab-4dd9-9d6f-66fedaf5e229",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Attention (Concatanative)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812176a6-56fc-4d4e-bc71-604f16c69381",
   "metadata": {},
   "source": [
    "## Attention (Luong)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
