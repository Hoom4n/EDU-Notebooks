{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b116852d-e1d2-4b12-9ba8-507aaf49f80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1658cf8-3c68-403f-b620-c2dae0d3e7d7",
   "metadata": {},
   "source": [
    "## Simple RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6aae5e6-f661-46cb-84d8-19787389b110",
   "metadata": {},
   "source": [
    "**Simple RNN Memory Cell**\n",
    "\n",
    "We can do computations in one shot for the whole layer:\n",
    "$$\n",
    "\\hat{Y}_t = \\phi(X_t W_x + \\hat{Y}_{t-1} W_{\\hat{y}} + b)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\hat{Y}_t$ is an $m \\times n_{\\text{neurons}}$ matrix containing the layerâ€™s outputs at time step $t$ for each instance in the mini-batch ($m$ is the number of instances in the mini-batch and $n_{\\text{neurons}}$ is the number of neurons).  \n",
    "- $X_t$ is an $m \\times n_{\\text{inputs}}$ matrix containing the inputs for all instances ($n_{\\text{inputs}}$ is the number of input features).  \n",
    "- $W_x$ is an $n_{\\text{inputs}} \\times n_{\\text{neurons}}$ matrix containing the connection weights for the inputs of the current time step.  \n",
    "- $W_{\\hat{y}}$ is an $n_{\\text{neurons}} \\times n_{\\text{neurons}}$ matrix containing the connection weights for the outputs of the previous time step.  \n",
    "- $b$ is a vector of size $n_{\\text{neurons}}$ containing each neuronâ€™s bias term.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9dab333-3446-4d60-a706-0a8a56f5471c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def simple_rnn_cell(x_t, h_t_1, w_x, w_h, b):\n",
    "    return tf.tanh(x_t @ w_x + h_t_1 @ w_h + b) # tensor in, tensor out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9abe2ab6-7fde-4be2-abd3-573c69bbbeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "units = 8\n",
    "n_features = 12\n",
    "timesteps = 16\n",
    "\n",
    "kernel_init = keras.initializers.glorot_uniform(seed=42) # best for tanh activation\n",
    "recurrent_init = keras.initializers.orthogonal(seed=42) # best for recurrent weights\n",
    "\n",
    "w_x = kernel_init(shape=(n_features, units))\n",
    "w_h = recurrent_init(shape=(units, units))\n",
    "b = tf.zeros(shape=(units)) # 1 bias per neuron\n",
    "h_init = tf.zeros(shape=(batch_size,units)) # if this was stateful rnn, we fed last hidden state of sequence before instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a1947ba-f472-46da-b5ba-5df2cb25dacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "input_sequence = tf.random.normal(shape=(batch_size,timesteps,n_features)) # typical 3D rnn input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2db9a718-a87d-48fc-b860-602f0eb9ca9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 8])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_t = input_sequence[:,0,:] # each timestep x(t) shape: (batch_size, n_features)\n",
    "h_t = simple_rnn_cell(x_t, h_init, w_x, w_h, b) # returns one hidden state per sample-neuron: (batch_size, units)\n",
    "y_t = h_t # in simple rnn, timestep t's output is equal to its hidden state\n",
    "y_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ceb68a2-7de3-4c23-a97c-e93838857212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 16, 8])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_t_1 = h_init\n",
    "hiddens_states = []\n",
    "\n",
    "for t in range(timesteps):\n",
    "    x_t = input_sequence[:,t,:] \n",
    "    h_t = simple_rnn_cell(x_t, h_t_1, w_x, w_h, b)\n",
    "    hiddens_states.append(h_t)\n",
    "    h_t_1 = h_t\n",
    "\n",
    "outputs = tf.stack(hiddens_states, axis=1) # stacked on axis=1, timesteps\n",
    "outputs.shape # returns (batch_size, timesteps, units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a380bc54-eccd-4f14-830b-2589750ea20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets wrap eveything in a function\n",
    "\n",
    "def simple_rnn(input_sequence, units, return_sequences=True, seed=42, h_init=None):\n",
    "    assert input_sequence.ndim == 3, \"RNN expects 3D input\"\n",
    "    batch_size, timesteps, n_features = input_sequence.shape\n",
    "\n",
    "    kernel_init = keras.initializers.glorot_uniform(seed=seed)\n",
    "    recurrent_init = keras.initializers.orthogonal(seed=seed)\n",
    "    w_x = kernel_init(shape=(n_features, units))\n",
    "    w_h = recurrent_init(shape=(units, units))\n",
    "    b = tf.zeros(shape=(units)) \n",
    "    h_init = tf.zeros(shape=(batch_size,units)) if h_init is None else h_init\n",
    "\n",
    "    h_t_1 = h_init\n",
    "    hiddens_states = []\n",
    "\n",
    "    for t in range(timesteps):\n",
    "        x_t = input_sequence[:,t,:] \n",
    "        h_t = simple_rnn_cell(x_t, h_t_1, w_x, w_h, b)\n",
    "        hiddens_states.append(h_t)\n",
    "        h_t_1 = h_t\n",
    "    \n",
    "    outputs = tf.stack(hiddens_states, axis=1)\n",
    "    \n",
    "    return outputs if return_sequences else outputs[:,-1,:] # or else return last timestep output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "735f752a-dde9-4160-9941-8d1a082ce424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are my results same as Keras layer? True\n"
     ]
    }
   ],
   "source": [
    "my_simple_rnn_outputs = simple_rnn(input_sequence, units, return_sequences=True, seed=42)\n",
    "\n",
    "keras_simple_rnn = keras.layers.SimpleRNN(units,\n",
    "                                          return_sequences=True, \n",
    "                                          kernel_initializer=kernel_init, \n",
    "                                          recurrent_initializer=recurrent_init)\n",
    "\n",
    "keras_simple_rnn_outputs = keras_simple_rnn(input_sequence)\n",
    "\n",
    "print(f\"Are my results same as Keras layer? {(my_simple_rnn_outputs == keras_simple_rnn_outputs).numpy().all()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304c4fff-5c54-4529-a7b7-ab7b25e0cc65",
   "metadata": {},
   "source": [
    "Yay!\n",
    "\n",
    "Some Notes:\n",
    "- Although those parameters need to be set up somehow so the optimizer can tweak them, I just implemented the forward pass to get the intuition.\n",
    "- A single Simple RNN cell outputs a tensor of shape (batch_size, features) at each time step, where the dimensionality of features equals the number of units. So essentially, at each time step, the hidden state of a single unit is a scalarâ€”meaning all the encoded information up to that point is compressed into a single number! Given the kinds of tasks RNNs can handle (speech recognition, image captioning, language modeling), this is quite fascinating!\n",
    "- If the gradients adjust the weights in a way that slightly increases them, it can lead to troubleâ€”because all time steps share the same weight matrix, those small changes can accumulate and eventually cause the hidden states to explode. Thatâ€™s one reason we use tanh, a saturated (bounded) activation function, instead of ReLU, which is unbounded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7455323d-79fb-4e04-adbf-4eedbb72f9a6",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02d2971-a7e9-4ef5-b69b-949354a2f93e",
   "metadata": {},
   "source": [
    "### Implementing LSTM Memory Cell: Step by Step in TensorFlow Eager Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d291ff-b64f-443c-9c02-f302b905b27d",
   "metadata": {},
   "source": [
    "According to Wikipedia:\n",
    "\n",
    "- $f_t = \\sigma(W_f x_t + U_f h_{t-1} + b_f)$  \n",
    "- $i_t = \\sigma(W_i x_t + U_i h_{t-1} + b_i)$  \n",
    "- $\\tilde{c}_t = \\tanh(W_c x_t + U_c h_{t-1} + b_c)$  \n",
    "- $o_t = \\sigma(W_o x_t + U_o h_{t-1} + b_o)$ \n",
    "- $c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t$\n",
    "- $h_t = o_t \\odot \\tanh(c_t)$ \n",
    "\n",
    "\n",
    "Initial values are $c_0 = 0$ \n",
    "and $h_0 = 0$, and the operator $\\odot$ denotes the Hadamard product (element-wise multiplication); the subscript $t$ indexes the time step.\n",
    "\n",
    "Letting the superscripts $d$ and $h$ refer to the number of input features and number of hidden units, respectively:\n",
    "\n",
    "- $x_t \\in \\mathbb{R}^d$: input vector to the LSTM unit  \n",
    "- $f_t \\in (0,1)^h$: forget gate's activation vector  \n",
    "- $i_t \\in (0,1)^h$: input/update gate's activation vector  \n",
    "- $o_t \\in (0,1)^h$: output gate's activation vector  \n",
    "- $h_t \\in (-1,1)^h$: hidden state vector (also output of the LSTM unit)  \n",
    "- $\\tilde{c}_t \\in (-1,1)^h$: cell input activation vector  \n",
    "- $c_t \\in \\mathbb{R}^h$: cell state vector  \n",
    "- $W \\in \\mathbb{R}^{h \\times d}$, $U \\in \\mathbb{R}^{h \\times h}$, $b \\in \\mathbb{R}^h$: weight matrices and bias vector  \n",
    "- $\\sigma_g$: sigmoid activation function  \n",
    "- $\\sigma_c$: hyperbolic tangent activation function\n",
    "\n",
    "these equations are for single sample, i will implement batched operations, so X_t is shaped: (batch_size, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0c741f3-750b-43fb-a9cd-a2a20f21b3aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 12])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "timesteps = 16\n",
    "n_features = 12\n",
    "units = 8\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "X = tf.random.normal(shape=(batch_size, timesteps, n_features)) # X batch: (batch_size, timesteps, n_features)\n",
    "x_t = X[:,0,:]\n",
    "x_t.shape # x_t shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39eb2384-9778-403c-8c80-8ff9c53949f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for now i have to initialize them seprately, but later on i will concatanate them and initialzie each of W, U once\n",
    "w_f = keras.initializers.GlorotUniform(seed=43)(shape=(units, n_features))\n",
    "w_i = keras.initializers.GlorotUniform(seed=44)(shape=(units, n_features))\n",
    "w_o = keras.initializers.GlorotUniform(seed=45)(shape=(units, n_features))\n",
    "w_c = keras.initializers.GlorotUniform(seed=46)(shape=(units, n_features))\n",
    "\n",
    "u_f = keras.initializers.Orthogonal(seed=53)(shape=(units, units))\n",
    "u_i = keras.initializers.Orthogonal(seed=54)(shape=(units, units))\n",
    "u_o = keras.initializers.Orthogonal(seed=55)(shape=(units, units))\n",
    "u_c = keras.initializers.Orthogonal(seed=56)(shape=(units, units))\n",
    "\n",
    "h_init = tf.zeros(shape=(batch_size, units))\n",
    "c_init = tf.zeros(shape=(batch_size, units))\n",
    "\n",
    "b_f = tf.ones(shape=(units)) # tensorflow init forget bias with ones, this prevents forgetting everything at the beginning of training\n",
    "b_i = tf.zeros(shape=(units))\n",
    "b_o = tf.zeros(shape=(units))\n",
    "b_c = tf.zeros(shape=(units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98fb18b5-cb28-4a1c-ab0f-8cc45775729c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 8])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x_t is (batch_size, n_features) and w_f is (units, n_features) -> matmul -> (batch_size, units)\n",
    "# h_t_1 is (batch_size, units) and u_f is (units, units) -> matmul -> (batch_size, units)\n",
    "# b is (units) -> broadcast and add\n",
    "\n",
    "h_t_1 = h_init\n",
    "\n",
    "f_t = tf.sigmoid(x_t @ tf.transpose(w_f)  + h_t_1 @ u_f + b_f) \n",
    "f_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f1f8db-03c3-4726-be28-c5087a92184b",
   "metadata": {},
   "source": [
    "Alright, \n",
    "ð‘“\n",
    "(\n",
    "ð‘¡\n",
    ")\n",
    " outputs \n",
    "(\n",
    "batch_size\n",
    ",\n",
    "Â units\n",
    ")\n",
    ". So for each sample and each unit, at each timestep, the forget activation is a number between 0 and 1 that determines how much of the memory cell state should be forgotten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "421da85e-8204-472b-b89c-fe2ac4e34cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of Input activation: (32, 8) and Output activation: (32, 8)\n"
     ]
    }
   ],
   "source": [
    "# same thing here\n",
    "\n",
    "i_t = tf.sigmoid(x_t @ tf.transpose(w_i)  + h_t_1 @ u_i + b_i) \n",
    "o_t = tf.sigmoid(x_t @ tf.transpose(w_o)  + h_t_1 @ u_o + b_o) \n",
    "\n",
    "print(f\"Shapes of Input activation: {i_t.shape} and Output activation: {o_t.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "293e7f26-c955-41e3-89d3-eb7ed54af9f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 8])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now c candiate. linear transformation + tanh, some of it will be added to long term memory\n",
    "c_candidate_t = tf.tanh(x_t @ tf.transpose(w_c)  + h_t_1 @ u_c + b_c) \n",
    "c_candidate_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20ca5138-6f38-4f55-a495-1fdbf4d0be18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 8])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now, forget and input gates: determine how much of c(t-1) should be forgotten and how much of the candidate cell state (c_candidate_t) should be added to it\n",
    "\n",
    "# forget gate: f_t (batch_size, units) * c_t_1 (batch_size, units)\n",
    "# input gate: i_t (batch_size, units) * c_candidate_t (batch_size, units)\n",
    "\n",
    "c_t_1 = c_init # initialize cell state with zeros\n",
    "\n",
    "c_t = f_t * c_t_1 + i_t * c_candidate_t\n",
    "\n",
    "c_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7b09105-8b5e-429f-afd6-ce3b684f4717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 8])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hiddens state h(t) (same as cell's output y(t)): for time step t is some of activated cell state\n",
    "\n",
    "h_t = y_t = o_t * tf.tanh( c_t )\n",
    "\n",
    "h_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d9c4b4-3ffa-4b7c-afe0-af11b89a4a9c",
   "metadata": {},
   "source": [
    "That's it!  \n",
    "This was for a single memory cell, so now let's write a function to handle all timesteps. We'll also concatenate the weight matrices and biases, then perform the matrix multiplications in one shot to take advantage of vectorization for a speed boost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce3b407-5f5c-4acb-945a-de49e74d4374",
   "metadata": {},
   "source": [
    "### Vectorizerd Implementation of LSTM Layer in TF Graph Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "832b4147-86e7-462d-ab15-db7cf5b8f0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacking all w matrices vertically, along first dimension -> W: (4 * units, n_features)\n",
    "# stacking all u matrices vertically, along first dimension -> U: (4 * units, units)\n",
    "# x_t: (batch_size, n_features)\n",
    "# h_t_1: (batch_size, units)\n",
    "# c_t_1: (batch_size, units)\n",
    "# b vector is repeated 4 times horizonaltly, -> B: (4 * units)\n",
    "\n",
    "@tf.function\n",
    "def lstm_cell(x_t, h_t_1, c_t_1, W, U, B):\n",
    "    z = x_t @ W + h_t_1 @ U + B # compute all 4 equations in one shot!\n",
    "    i_t, f_t, c_candidate_t, o_t = tf.split(z, 4, axis=1)\n",
    "    \n",
    "    f_t = tf.sigmoid(f_t)\n",
    "    i_t = tf.sigmoid(i_t)\n",
    "    o_t = tf.sigmoid(o_t)\n",
    "    c_candidate_t = tf.tanh(c_candidate_t)\n",
    "    \n",
    "    c_t = f_t * c_t_1 + i_t * c_candidate_t\n",
    "    h_t = o_t * tf.tanh(c_t)\n",
    "    return h_t, c_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53abc18c-616c-47c8-8eed-3450664ee097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm(input_sequence, units, kernel_initializer, recurrent_initializer, return_sequences=True, h_init=None, c_init=None, return_state=False):\n",
    "    assert input_sequence.ndim == 3, \"LSTM expects 3D input\"\n",
    "    batch_size, timesteps, n_features = input_sequence.shape\n",
    "\n",
    "    W_init = kernel_init(shape=(n_features, 4 * units)) \n",
    "    U_init = recurrent_init(shape=(units, 4 * units))   \n",
    "    \n",
    "    b_init = tf.concat([\n",
    "        tf.zeros(units),  # Input gate\n",
    "        tf.ones(units),   # Forget gate\n",
    "        tf.zeros(units),  # Cell candidate\n",
    "        tf.zeros(units)   # Output gate\n",
    "                        ], axis=0)\n",
    "    \n",
    "    h_init = tf.zeros(shape=(batch_size,units)) if h_init is None else h_init\n",
    "    c_init = tf.zeros(shape=(batch_size,units)) if c_init is None else c_init\n",
    "    \n",
    "    W = W_init\n",
    "    U = U_init\n",
    "    B = b_init\n",
    "    h_t_1 = h_init \n",
    "    c_t_1 = c_init\n",
    "\n",
    "    hiddens_states = []\n",
    "    cell_states = []\n",
    "    \n",
    "    for t in range(timesteps):\n",
    "        x_t = input_sequence[:,t,:] \n",
    "        h_t, c_t = lstm_cell(x_t, h_t_1, c_t_1, W, U, B)\n",
    "        \n",
    "        hiddens_states.append(h_t)\n",
    "        cell_states.append(c_t)\n",
    "        \n",
    "        h_t_1 = h_t\n",
    "        c_t_1 = c_t\n",
    "    \n",
    "    layer_hidden_states = tf.stack(hiddens_states, axis=1)\n",
    "\n",
    "    if return_state:\n",
    "        return layer_hidden_states, h_t, c_t\n",
    "    elif return_sequences:\n",
    "        return layer_hidden_states  \n",
    "    else:\n",
    "        return layer_hidden_states[:, -1, :]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d7bbab1-0436-4b4d-900f-23e49b9879aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "input_sequence = tf.random.normal(shape=(batch_size,timesteps,n_features)) \n",
    "\n",
    "kernel_init = keras.initializers.glorot_uniform(seed=42)\n",
    "recurrent_init = keras.initializers.orthogonal(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb105e64-065d-477f-9dda-a924b5fd79ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are they same?: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "my_lstm_hidden_states = lstm(input_sequence, units, kernel_init, recurrent_init, return_sequences=True, h_init=None, c_init=None, return_state=False)\n",
    "\n",
    "keras_lstm_layer = keras.layers.LSTM(units,\n",
    "                                     kernel_initializer=kernel_init, recurrent_initializer=recurrent_init,\n",
    "                                     return_sequences=True, return_state=False, use_cudnn=False)\n",
    "\n",
    "keras_lstm_hidden_states = keras_lstm_layer(input_sequence)\n",
    "\n",
    "print(f\"Are they same?: {(my_lstm_hidden_states == keras_lstm_hidden_states).numpy().all()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3528a3-6b37-43b4-8e70-da1c4923d421",
   "metadata": {},
   "source": [
    "Yayy!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6c46f7-3b02-4af2-8a3f-dbd0fe7bbb39",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af0e754-0cab-4dd9-9d6f-66fedaf5e229",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Attention (Concatanative)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812176a6-56fc-4d4e-bc71-604f16c69381",
   "metadata": {},
   "source": [
    "## Attention (Luong)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
